{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bce752-40b3-4d25-964a-aa952aa5d112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "hf_token = \"TOKEN\"\n",
    "hf_repo_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "BASE_ARTIFACT_URI = \"gs://BUCKET\"\n",
    "os.system(f\"rm -rf /tmp/model\")\n",
    "print(\"Start downloading\")\n",
    "snapshot_download(repo_id=hf_repo_id, token=hf_token, local_dir=f\"/tmp/model\")\n",
    "print(\"Uploading\")\n",
    "os.system(f\"gcloud storage cp /tmp/model/*.* {BASE_ARTIFACT_URI}/{hf_repo_id}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab33c0-06f1-4f54-8683-466da9c0ba3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "#Add Pub/Sub publisher to this account for topic tgi\n",
    "PROJECT_NUMBER = \"NUMBER\"\n",
    "PROJECT_ID = \"ID\"\n",
    "SERVICE_ACCOUNT = \"SA\"\n",
    "MODEL_PATH = \"gs://MODEL_PATH\"\n",
    "CUSTOM_CONTAINER = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/custom-inference-gpu/tgi-gcp\"\n",
    "#Check https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel#google_cloud_aiplatform_prediction_LocalModel\n",
    "#Refer https://huggingface.co/docs/text-generation-inference/en/reference/launcher  for serving_container_args\n",
    "#Refer https://github.com/huggingface/Google-Cloud-Containers/blob/main/containers/tgi/gpu/2.4.0/entrypoint.sh to check entry point\n",
    "#Refer https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables for monitor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955472d-a148-4f25-aa54-70d544d94a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CONTAINER = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311\"\n",
    "CONTAINER = CUSTOM_CONTAINER\n",
    "!docker build -t {CONTAINER} .\n",
    "MODEL_NAME = \"Llama-3.1-8B-Instruct\"\n",
    "\n",
    "#must secure sufficient space\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "local_model = LocalModel(serving_container_image_uri=CONTAINER,\n",
    "                         serving_container_environment_variables={\n",
    "                             \"VERTEX_CPR_MAX_WORKERS\": \"1\",\n",
    "                             \"RUST_BACKTRACE\": \"full\", #for stack trace printing,\n",
    "                             #\"PORT\": \"5000\", #server runs on 5000, or 8080 by dafault\n",
    "                         },\n",
    "                         #serving_container_ports=[5000], #expose container port, system map is random\n",
    "                         serving_container_health_route=\"/aiphealth\", #need to add bypass conflict only for localmodel\n",
    "                         serving_container_args=[\"--num-shard 1\"], #We can use both serving_container_environment_variables and serving_container_args\n",
    "                        )\n",
    "\n",
    "#TGI container handles /generate as input handler\n",
    "prediction_handler = \"/generate\"\n",
    "prediction_input = {\n",
    "    \"inputs\": \"What is machine learning?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692006a-a96f-459c-b263-778ff57305e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTAINER = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/vllm-inference.cu121.0-5.ubuntu2204.py310\"\n",
    "MODEL_NAME = \"Llama-3.1-8B-Instruct-vllm-prebuilt-container\"\n",
    "\n",
    "#must secure sufficient space\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "local_model = LocalModel(serving_container_image_uri=CONTAINER,\n",
    "                        serving_container_ports=[8080], #expose container port, system map is random\n",
    "                        serving_container_health_route=\"/ping\",\n",
    "                        serving_container_predict_route=\"/generate\",\n",
    "                        serving_container_args=[\"python\",\n",
    "                                               \"-m\",\n",
    "                                               \"vllm.entrypoints.api_server\",\n",
    "                                               \"--host=0.0.0.0\",\n",
    "                                               \"--port=8080\",\n",
    "                                               \"--gpu-memory-utilization=0.9\",\n",
    "                                               \"--max-model-len=16384\"]\n",
    "                        )\n",
    "\n",
    "#Default input handler\n",
    "prediction_handler = \"/predict\"\n",
    "prediction_input = {\n",
    "    \"instances\" : [\n",
    "        {\n",
    "          \"prompt\": \"What is machine learning?\"\n",
    "        }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"repetition_penalty\": 1.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48df827-d978-4b28-892f-13d5f67e278a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30303a-87d0-41cd-b116-98a2682d6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run conainer locally to check base output\n",
    "#docker run -it -p 5000:5000 -e AIP_HTTP_PORT=5000 -e AIP_STORAGE_URI=gs://jk-model-repo/gemma1/gemma-2b-it-test --gpus=all us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d051ea8-b62a-44b7-8509-3e86edfd22cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test plain endpoint\n",
    "import json\n",
    "import time\n",
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=MODEL_PATH,\n",
    "    gpu_count=-1,\n",
    "    container_ready_timeout = 600\n",
    ") as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    local_endpoint.serving_container_predict_route = prediction_handler\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=json.dumps(prediction_input),\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )    \n",
    "    time.sleep(30)\n",
    "    local_endpoint.print_container_logs()\n",
    "print(health_check_response, health_check_response.content)\n",
    "print(predict_response.text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bfae56d-abcc-4ba5-9459-155e3cc9f0d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "import json\n",
    "#Manual deploy and test\n",
    "local_endpoint = local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=MODEL_PATH,\n",
    "    gpu_count=-1,\n",
    "    container_ready_timeout = 600)\n",
    "local_endpoint.serve()\n",
    "local_endpoint.print_container_logs()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5da449ee-6aba-48b2-93c1-7d7ffa911957",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Run some code here to test monitor\n",
    "local_endpoint.serving_container_predict_route = prediction_handler\n",
    "predict_response = local_endpoint.predict(\n",
    "        request=json.dumps(prediction_input),\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "print(predict_response.text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ec26794-d770-4605-972f-30f074a03d9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "local_endpoint.stop()\n",
    "local_endpoint.print_container_logs()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79085489-31c2-4adf-b398-ce1dce71d53a",
   "metadata": {},
   "source": [
    "!gcloud artifacts repositories create {REPOSITORY} --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b802e-b448-4070-ae3e-df45c6adbb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run if needed\n",
    "!gcloud auth configure-docker us-central1-docker.pkg.dev --quiet\n",
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a475f1-3143-4334-960f-0b5dc80f2f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"{MODEL_NAME} test endpoint\",\n",
    "    labels={\"sample-key\": \"sample-value\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8fa9f9-8773-423c-ac45-0ee28b70a126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name = MODEL_NAME,\n",
    "    local_model = local_model,\n",
    "    artifact_uri = MODEL_PATH,\n",
    "    #parent_model = prev_model.resource_name,\n",
    "    #is_default_version=True,\n",
    "    #serving_container_environment_variables={\n",
    "    #    \"VERTEX_CPR_MAX_WORKERS\": \"1\",\n",
    "    #    \"PORT\": \"8080\", #server runs on 5000, or 8080 by dafault\n",
    "    #    \"RUST_BACKTRACE\": \"full\", #for stack trace printing,\n",
    "    #},\n",
    "    #serving_container_ports=[8080],\n",
    "    #serving_container_args = [\"--num-shard 1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807aa10f-fe7b-41b8-b8fe-34fe3d592b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint.deploy(\n",
    "    model = model,\n",
    "    machine_type=\"g2-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    #accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    service_account=SERVICE_ACCOUNT\n",
    "    #traffic_percentage=50\n",
    "    #traffic_split={'a':50, 'b':50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b582ea4-2b79-4c8f-ba1e-2fe5e10b98cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run this if start from endpoint id\n",
    "ENDPOINT_ID = \"00000000000\"\n",
    "from google.cloud import aiplatform\n",
    "endpoint = aiplatform.Endpoint(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19e19d-0e8d-454a-9b70-208821b002d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = endpoint.predict(\n",
    "    instances=[\n",
    "        {\n",
    "            \"inputs\": \"<bos><start_of_turn>user\\nWhat's Deep Learning?<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 128,\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.95,\n",
    "                \"temperature\": 0.7,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(output.predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886afd6-133c-458c-ba95-1be308803af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library\n",
    "#https://cloud.google.com/deep-learning-containers/docs/choosing-container#vLLM-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b676309-bcc1-4902-b932-d4458f19b1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import requests\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_NUMBER}/locations/us-central1/endpoints/{ENDPOINT_ID}:predict\"\n",
    "headers = {'Authorization': f'Bearer {creds.token}'}\n",
    "payload = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"inputs\": \"Hello!\",\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 128\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "response = requests.post(url, json=payload, headers=headers).json()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c4cbf-eee2-4009-9bfe-c31d2f61026f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Chat and streaming failure needs to be checked on TGI container, vllm container worked\n",
    "import google.auth\n",
    "import requests\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_NUMBER}/locations/us-central1/endpoints/{ENDPOINT_ID}/chat/completions\"\n",
    "headers = {'Authorization': f'Bearer {creds.token}'}\n",
    "payload = {\n",
    "    \"stream\": True,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a story about a magic backpack.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "response = requests.post(url, json=payload, headers=headers).json()\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu118.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
