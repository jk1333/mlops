{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bce752-40b3-4d25-964a-aa952aa5d112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "hf_token = \"----\"\n",
    "hf_repo_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "BASE_ARTIFACT_URI = \"gs://----\"\n",
    "os.system(f\"rm -rf /tmp/model\")\n",
    "print(\"Start downloading\")\n",
    "snapshot_download(repo_id=hf_repo_id, token=hf_token, local_dir=f\"/tmp/model\")\n",
    "print(\"Uploading\")\n",
    "os.system(f\"gcloud storage cp /tmp/model/*.* {BASE_ARTIFACT_URI}/{hf_repo_id}\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640104d-8ea0-48d4-b6ce-7a792996aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker rm -vf $(docker ps -aq)\n",
    "!docker rmi -f $(docker images -aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab33c0-06f1-4f54-8683-466da9c0ba3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOCATION = \"us-central1\"\n",
    "PROJECT_NUMBER = \"----\"\n",
    "PROJECT_ID = \"----\"\n",
    "SECOND_PROJECT_ID = \"----\"\n",
    "MODEL_PATH_V1 = \"gs://jk-model-repo/meta-llama/Llama-3.1-8B-Instruct\"\n",
    "MODEL_PATH_V2 = \"gs://jk-model-repo/meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#VPC_NETWORK = \"globalnetwork\" #vpc network name to peering\n",
    "#Check https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel#google_cloud_aiplatform_prediction_LocalModel\n",
    "#Refer https://huggingface.co/docs/text-generation-inference/en/reference/launcher  for serving_container_args\n",
    "#Refer https://github.com/huggingface/Google-Cloud-Containers/blob/main/containers/tgi/gpu/2.4.0/entrypoint.sh to check entry point\n",
    "#Refer https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables for monitor variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb830a4-9ebc-48db-8867-a77aeaf9a976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To run vllm\n",
    "CONTAINER = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/custom-inference-gpu/vllm-release:latest\"\n",
    "!docker image tag us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/vllm-inference.cu121.0-5.ubuntu2204.py310:latest {CONTAINER}\n",
    "MODEL_NAME_V1 = \"Llama-3.1-8B-Instruct-VLLM\"\n",
    "\n",
    "#must secure sufficient space\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "local_model_v1 = LocalModel(serving_container_image_uri=CONTAINER,\n",
    "                        serving_container_ports=[8080], #expose container port, system map is random\n",
    "                        serving_container_health_route=\"/metrics\",\n",
    "                        serving_container_predict_route=\"/v1/chat/completions\",\n",
    "                        serving_container_args=[\"python\",\n",
    "                                               \"-m\",\n",
    "                                               \"vllm.entrypoints.openai.api_server\",\n",
    "                                               \"--host=0.0.0.0\",\n",
    "                                               \"--port=8080\",\n",
    "                                               \"--gpu-memory-utilization=0.9\",\n",
    "                                               \"--max-model-len=16384\"]\n",
    "                        )\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class CarType(str, Enum):\n",
    "    sedan = \"sedan\"\n",
    "    suv = \"SUV\"\n",
    "    truck = \"Truck\"\n",
    "    coupe = \"Coupe\"\n",
    "\n",
    "class CarDescription(BaseModel):\n",
    "    brand: str\n",
    "    model: str\n",
    "    car_type: CarType\n",
    "\n",
    "json_schema = CarDescription.model_json_schema()\n",
    "\n",
    "prediction_input = {\n",
    "    \"model\": \"openapi\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Generate a JSON with the brand, model and car_type of the most iconic car from the 90's\"\n",
    "        }\n",
    "    ],\n",
    "    \"guided_json\": json_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "178412c7-d7b6-47a5-8ec9-40c0c6a54f40",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#To run TGI\n",
    "#CONTAINER = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-4.ubuntu2204.py311\"\n",
    "CONTAINER = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/custom-inference-gpu/tgi-release:latest\"\n",
    "!docker build -t {CONTAINER} .\n",
    "MODEL_NAME_V1 = \"Llama-3.1-8B-Instruct-TGI\"\n",
    "\n",
    "#must secure sufficient space\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "local_model_v1 = LocalModel(serving_container_image_uri=CONTAINER,\n",
    "                         serving_container_environment_variables={\n",
    "                             \"VERTEX_CPR_MAX_WORKERS\": \"1\",\n",
    "                             \"RUST_BACKTRACE\": \"full\", #for stack trace printing,\n",
    "                             \"CUDA_MEMORY_FRACTION\": \"0.93\",\n",
    "                             #\"AIP_PREDICT_ROUTE\": \"/generate\",\n",
    "                             #\"AIP_HEALTH_ROUTE\": \"/metrics\"\n",
    "                             #\"MODEL_ID\": f\"meta-llama/{MODEL_NAME}\"\n",
    "                             #\"PORT\": \"5000\", #server runs on 5000, or 8080 by dafault\n",
    "                         },\n",
    "                         #serving_container_ports=[5000], #expose container port, system map is random\n",
    "                         serving_container_health_route=\"/metrics\",\n",
    "                         serving_container_predict_route=\"/generate\",\n",
    "                         serving_container_args=[\"--num-shard 1\"], #We can use both serving_container_environment_variables and serving_container_args\n",
    "                        )\n",
    "\n",
    "from pydantic import BaseModel, conint\n",
    "from typing import List\n",
    "class Animals(BaseModel):\n",
    "    location: str\n",
    "    activity: str\n",
    "    animals_seen: conint(ge=1, le=5)  # Constrained integer type\n",
    "    animals: List[str]\n",
    "    \n",
    "prompt = \"convert to JSON: I saw a puppy a cat and a raccoon during my bike ride in the park\"\n",
    "\n",
    "prediction_input = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"repetition_penalty\": 1.3,\n",
    "        \"grammar\": {\n",
    "            \"type\": \"json\",\n",
    "            \"value\": Animals.model_json_schema()\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48df827-d978-4b28-892f-13d5f67e278a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model_v1.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae8e93-9149-47a0-94d6-a5e68e8ccdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#Manual deploy and test\n",
    "local_endpoint = local_model_v1.deploy_to_local_endpoint(\n",
    "    artifact_uri=MODEL_PATH_V1,\n",
    "    gpu_count=-1,\n",
    "    container_ready_timeout = 600)\n",
    "local_endpoint.serve()\n",
    "local_endpoint.print_container_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24df91-035f-4320-9635-54a0869d25c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run some code here to test monitor\n",
    "predict_response = local_endpoint.predict(\n",
    "        request=json.dumps(prediction_input),\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "print(predict_response.text)\n",
    "local_endpoint.print_container_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de672cc2-0813-4b93-987c-9fd0e668fc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_endpoint.run_health_check().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea162c58-b6d6-4f13-95f4-020728fca78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_endpoint.stop()\n",
    "local_endpoint.print_container_logs()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79085489-31c2-4adf-b398-ce1dce71d53a",
   "metadata": {},
   "source": [
    "!gcloud artifacts repositories create {REPOSITORY} --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b802e-b448-4070-ae3e-df45c6adbb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run if needed for serving container update\n",
    "!gcloud auth configure-docker us-central1-docker.pkg.dev --quiet\n",
    "local_model_v1.push_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8fa9f9-8773-423c-ac45-0ee28b70a126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "model_v1 = aiplatform.Model.upload(\n",
    "    display_name = MODEL_NAME_V1,\n",
    "    local_model = local_model_v1,\n",
    "    artifact_uri = MODEL_PATH_V1,\n",
    "    #parent_model = prev_model.resource_name,\n",
    "    #is_default_version=True,\n",
    "    #serving_container_environment_variables={\n",
    "    #    \"VERTEX_CPR_MAX_WORKERS\": \"1\",\n",
    "    #    \"PORT\": \"8080\", #server runs on 5000, or 8080 by dafault\n",
    "    #    \"RUST_BACKTRACE\": \"full\", #for stack trace printing,\n",
    "    #},\n",
    "    #serving_container_ports=[8080],\n",
    "    #serving_container_args = [\"--num-shard 1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7f34ee9-123f-4fa2-90f2-02c0d16bc79e",
   "metadata": {},
   "source": [
    "#Public and dedicated endpoint\n",
    "from google.cloud import aiplatform\n",
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"{MODEL_NAME} proxy public test endpoint\",\n",
    "    labels={\"sample-key\": \"sample-value\"},\n",
    "    #dedicated_endpoint_enabled=True,\n",
    ")\n",
    "endpoint.deploy(\n",
    "    model = model,\n",
    "    machine_type=\"g2-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    #accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    #service_account=SERVICE_ACCOUNT\n",
    "    #traffic_percentage=50\n",
    "    #traffic_split={'a':50, 'b':50}\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eba31a95-e3ab-4932-8fb0-224182a1aa95",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Public and dedicated endpoint predict\n",
    "from google.cloud import aiplatform\n",
    "#ENDPOINT_ID = \"0000\"\n",
    "#endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
    "response = endpoint.raw_predict(body=json.dumps(prediction_input, indent=2).encode('utf-8'), headers={'Content-Type':'application/json'})\n",
    "response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a60ca0-a166-4ef4-9d3f-f289b1c816f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Private endpoint with PSC\n",
    "#Refer https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints\n",
    "from google.cloud import aiplatform\n",
    "endpoint = aiplatform.PrivateEndpoint.create(\n",
    "    display_name=f\"{MODEL_NAME_V1} proxy private test endpoint\",\n",
    "    private_service_connect_config=aiplatform.PrivateEndpoint.PrivateServiceConnectConfig(\n",
    "        project_allowlist=[PROJECT_ID, SECOND_PROJECT_ID],\n",
    "    ),\n",
    "    #network=f\"projects/{PROJECT_NUMBER}/global/networks/{VPC_NETWORK}\",\n",
    "    labels={\"sample-key\": \"sample-value\"},\n",
    ")\n",
    "#C3, L4, TPU not allowed for private endpoint\n",
    "#Refer https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
    "endpoint.deploy(\n",
    "    model = model_v1,\n",
    "    machine_type=\"g2-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    #accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1, #Set this value means do autoscaling\n",
    "    #service_account=SERVICE_ACCOUNT\n",
    "    #traffic_percentage=50\n",
    "    #traffic_split={'a':50, 'b':50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376345c0-cd0b-4c19-9915-7ecbf05c9912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint.gca_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9280e-2231-40dd-b85e-4bd2724bbe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_attachment = endpoint.list_models()[0].private_endpoints.service_attachment\n",
    "print(service_attachment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a746dbf-b182-442f-8f02-bc89cc31b156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud compute addresses create psc-prediction \\\n",
    "    --region=us-central1 \\\n",
    "    --subnet=subnet4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcb3c0-5f1b-41e0-8044-b36422ffb908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud compute forwarding-rules create op-psc-endpoint \\\n",
    "    --network=globalnetwork \\\n",
    "    --address=psc-prediction \\\n",
    "    --target-service-attachment={service_attachment} \\\n",
    "    --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e6cf5-06c6-49b7-be38-e6844697fd43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IP_ADDRESS = ! gcloud compute forwarding-rules describe op-psc-endpoint --region=us-central1 --format='value(IPAddress)'\n",
    "IP_ADDRESS = IP_ADDRESS[0]\n",
    "print(IP_ADDRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d356e-a5cd-4530-9da6-1a7a66b45e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Private endpoint with raw predict\n",
    "from google.cloud import aiplatform\n",
    "#ENDPOINT_ID = \"1745348265257205760\"\n",
    "#endpoint = aiplatform.PrivateEndpoint(ENDPOINT_ID)\n",
    "response = endpoint.raw_predict(body=json.dumps(prediction_input, indent=2).encode('utf-8'), headers={'Content-Type':'application/json'}, \n",
    "                               endpoint_override=IP_ADDRESS)\n",
    "json.loads(response.data)['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49654ac-1da2-48fc-8b80-9afdb8bbbb38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME_V2 = \"Llama-3.1-8B-Instruct-VLLM-V1\"\n",
    "local_model_v2 = LocalModel(serving_container_image_uri=CONTAINER,\n",
    "                        serving_container_ports=[8080], #expose container port, system map is random\n",
    "                        serving_container_health_route=\"/metrics\",\n",
    "                        serving_container_predict_route=\"/v1/chat/completions\",\n",
    "                        serving_container_args=[\"python\",\n",
    "                                               \"-m\",\n",
    "                                               \"vllm.entrypoints.openai.api_server\",\n",
    "                                               \"--host=0.0.0.0\",\n",
    "                                               \"--port=8080\",\n",
    "                                               \"--gpu-memory-utilization=0.9\",\n",
    "                                               \"--max-model-len=16384\"]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbd9dc-be21-445a-877b-6d2314bbd241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test plain endpoint\n",
    "import json\n",
    "import time\n",
    "with local_model_v2.deploy_to_local_endpoint(\n",
    "    artifact_uri=MODEL_PATH_V2,\n",
    "    gpu_count=-1,\n",
    "    container_ready_timeout = 600\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=json.dumps(prediction_input),\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    time.sleep(30)\n",
    "    local_endpoint.print_container_logs()\n",
    "print(health_check_response, health_check_response.content)\n",
    "print(predict_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89b1c7-a2f4-4093-9b69-7d2422709ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown, display\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.preview.evaluation import EvalTask\n",
    "from vertexai.preview.evaluation.metrics import (\n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate,\n",
    "    TrajectorySingleToolUse,\n",
    ")\n",
    "\n",
    "def display_eval_report(eval_result: pd.DataFrame) -> None:\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "    metrics_df = pd.DataFrame.from_dict(eval_result.summary_metrics, orient=\"index\").T\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(metrics_df)\n",
    "\n",
    "    display(Markdown(f\"### Row-wise Metrics\"))\n",
    "    display(eval_result.metrics_table)\n",
    "    \n",
    "prompt = [\n",
    "    \"Turn device_2 power off\", # example 1\n",
    "    \"Get user_x preference temperature and set Living Room temperature to the preferred value\", # example 2\n",
    "    \"Get user_y preference temperature and set Master Room temperature to the preferred value\", # example 3\n",
    "    \"Set all devices off\" # example 4\n",
    "]\n",
    "\n",
    "reference_trajectory = [\n",
    "# example 1\n",
    "[\n",
    "  {\n",
    "    \"tool_name\": \"set_device_info\",\n",
    "    \"tool_input\": {\n",
    "        \"device_id\": \"device_2\",\n",
    "        \"updates\": {\n",
    "            \"status\": \"OFF\"\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "],\n",
    "# example 2\n",
    "[\n",
    "    {\n",
    "      \"tool_name\": \"get_user_preferences\",\n",
    "      \"tool_input\": {\n",
    "          \"user_id\": \"user_x\"\n",
    "      }\n",
    "  },\n",
    "  {\n",
    "      \"tool_name\": \"set_temperature\",\n",
    "      \"tool_input\": {\n",
    "          \"location\": \"Living Room\",\n",
    "          \"temperature\": 23\n",
    "      }\n",
    "    },\n",
    "],\n",
    "# example 3\n",
    "[\n",
    "    {\n",
    "      \"tool_name\": \"get_user_preferences\",\n",
    "      \"tool_input\": {\n",
    "          \"user_id\": \"user_y\"\n",
    "      }\n",
    "  },\n",
    "  {\n",
    "      \"tool_name\": \"set_temperature\",\n",
    "      \"tool_input\": {\n",
    "          \"location\": \"Master Room\",\n",
    "          \"temperature\": 26\n",
    "      }\n",
    "    },\n",
    "],\n",
    "# example 4\n",
    "[\n",
    "  {\n",
    "    \"tool_name\": \"set_device_info\",\n",
    "    \"tool_input\": {\n",
    "        \"device_id\": \"device_1\",\n",
    "        \"updates\": {\n",
    "            \"status\": \"OFF\"\n",
    "        }\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"tool_name\": \"set_device_info\",\n",
    "    \"tool_input\": {\n",
    "        \"device_id\": \"device_2\",\n",
    "        \"updates\": {\n",
    "            \"status\": \"OFF\"\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "]\n",
    "\n",
    "predicted_trajectory = [\n",
    "# example 1\n",
    "[\n",
    "  {\n",
    "    \"tool_name\": \"set_device_info\",\n",
    "    \"tool_input\": {\n",
    "        \"device_id\": \"device_3\",\n",
    "        \"updates\": {\n",
    "            \"status\": \"OFF\"\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "],\n",
    "# example 2\n",
    "[\n",
    "    {\n",
    "      \"tool_name\": \"get_user_preferences\",\n",
    "      \"tool_input\": {\n",
    "          \"user_id\": \"user_z\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"tool_name\": \"set_temperature\",\n",
    "      \"tool_input\": {\n",
    "          \"location\": \"Living Room\",\n",
    "          \"temperature\": 23\n",
    "      }\n",
    "    },\n",
    "],\n",
    "# example 3, does not care about input parameter order\n",
    "[\n",
    "    {\n",
    "      \"tool_name\": \"get_user_preferences\",\n",
    "      \"tool_input\": {\n",
    "          \"user_id\": \"user_y\"\n",
    "      }\n",
    "  },\n",
    "  {\n",
    "      \"tool_name\": \"set_temperature\",\n",
    "      \"tool_input\": {          \n",
    "          \"temperature\": 26,\n",
    "          \"location\": \"Master Room\"\n",
    "      }\n",
    "    },\n",
    "],\n",
    "# example 4, add additional device in route\n",
    "[\n",
    "  {\n",
    "    \"tool_name\": \"set_device_info\",\n",
    "    \"tool_input\": {\n",
    "        \"device_id\": \"device_2\",\n",
    "        \"updates\": {\n",
    "            \"status\": \"OFF\"\n",
    "        }\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"tool_name\": \"set_device_info\",\n",
    "    \"tool_input\": {\n",
    "        \"device_id\": \"device_1\",\n",
    "        \"updates\": {\n",
    "            \"status\": \"OFF\"\n",
    "        }\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"tool_name\": \"set_device_info\",\n",
    "    \"tool_input\": {\n",
    "        \"device_id\": \"device_3\",\n",
    "        \"updates\": {\n",
    "            \"status\": \"OFF\"\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "]\n",
    "\n",
    "response = [\n",
    "    \"Device 3 power off\",\n",
    "    \"Set Living Room temperature to 23 celcius\",\n",
    "    \"Set Master Room temperature to 26 celcius\",\n",
    "    \"All devices turned off\"\n",
    "]\n",
    "\n",
    "eval_dataset = pd.DataFrame({\n",
    "    \"prompt\": prompt,\n",
    "    \"predicted_trajectory\": predicted_trajectory,\n",
    "    \"reference_trajectory\": reference_trajectory,\n",
    "    \"response\": response\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7fbfd-0209-4d63-b8d9-c0187810aea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b7f19-2b27-4e23-8386-ad69ebd4b9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criteria = {\n",
    "    \"Follows trajectory\": (\n",
    "        \"Evaluate whether the agent's response logically follows from the \"\n",
    "        \"sequence of actions it took. Consider these sub-points:\\n\"\n",
    "        \"  - Does the response reflect the information gathered during the trajectory?\\n\"\n",
    "        \"  - Is the response consistent with the goals and constraints of the task?\\n\"\n",
    "        \"  - Are there any unexpected or illogical jumps in reasoning?\\n\"\n",
    "        \"Provide specific examples from the trajectory and response to support your evaluation.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "pointwise_rating_rubric = {\n",
    "    \"1\": \"Follows trajectory\",\n",
    "    \"0\": \"Does not follow trajectory\",\n",
    "}\n",
    "\n",
    "response_follows_trajectory_prompt_template = PointwiseMetricPromptTemplate(\n",
    "    criteria=criteria,\n",
    "    rating_rubric=pointwise_rating_rubric,\n",
    "    input_variables=[\"prompt\", \"predicted_trajectory\"],\n",
    ")\n",
    "print(response_follows_trajectory_prompt_template.prompt_data)\n",
    "response_follows_trajectory_metric = PointwiseMetric(\n",
    "    metric=\"response_follows_trajectory\",\n",
    "    metric_prompt_template=response_follows_trajectory_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae5e00-0791-41b7-9956-eb6aa00664b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.evaluation import EvalTask\n",
    "eval_task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[\n",
    "        \"trajectory_exact_match\", # check exactly same 0/1\n",
    "        \"trajectory_in_order_match\", # check order matched and have extra functions\n",
    "        \"trajectory_any_order_match\", # check order not matched and have extra functions\n",
    "        \"trajectory_precision\", #0-1, higher is better, (count(predicted found in reference))/(total number of actions in predicted)\n",
    "        \"trajectory_recall\", #0-1, higher is better, (count(reference found in predicted))/(total number of actions in reference)\n",
    "        \"safety\",\n",
    "        response_follows_trajectory_metric\n",
    "    ],\n",
    ")\n",
    "\n",
    "#Use runnable if dynamic generation required, this will generates latency and failure parts\n",
    "eval_result = eval_task.evaluate(\n",
    "    #runnable=RUNNABLE,\n",
    ")\n",
    "\n",
    "display_eval_report(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b706e-4688-4e2e-8370-c7ef163e9138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make model v2\n",
    "model_v2 = aiplatform.Model.upload(\n",
    "    display_name = MODEL_NAME_V2,\n",
    "    local_model = local_model_v2,\n",
    "    artifact_uri = MODEL_PATH_V2,\n",
    "    parent_model = model_v1.resource_name,\n",
    "    #is_default_version=True,\n",
    "    #serving_container_environment_variables={\n",
    "    #    \"VERTEX_CPR_MAX_WORKERS\": \"1\",\n",
    "    #    \"PORT\": \"8080\", #server runs on 5000, or 8080 by dafault\n",
    "    #    \"RUST_BACKTRACE\": \"full\", #for stack trace printing,\n",
    "    #},\n",
    "    #serving_container_ports=[8080],\n",
    "    #serving_container_args = [\"--num-shard 1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d46850-c2d9-4dd8-b6b0-1c02b8e19020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#deploy second model (use sameone for testing)\n",
    "endpoint.deploy(\n",
    "    model = model_v2,\n",
    "    machine_type=\"g2-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    #machine_type=\"a2-highgpu-1g\",\n",
    "    #accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    #service_account=SERVICE_ACCOUNT\n",
    "    traffic_percentage=50\n",
    "    #traffic_split={'a':50, 'b':50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d42c8d-cbf1-4928-a420-6fdacfe3d35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_info in endpoint.list_models():\n",
    "    print(\"----------------------------\")\n",
    "    print(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8694899-8c9e-4c7c-80df-80625054ebc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Traffic to v2 all\n",
    "endpoint = endpoint.update(\n",
    "    traffic_split={\n",
    "                    \"[DEPLOYED MODELID(Not model repository)]\": 100,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "678eae75-75c0-4fe2-be9e-a82b83b66117",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Private endpoint health (PSC not support)\n",
    "import google.auth\n",
    "import requests\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "headers = {'Authorization': f'Bearer {creds.token}'}\n",
    "response = requests.get(endpoint.health_http_uri, headers=headers)\n",
    "#print(response.headers)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76fbc7-12bd-436a-bcbf-e6f188394a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Custom network timeout endpoint\n",
    "from google.cloud import aiplatform\n",
    "import urllib3\n",
    "class CustomPrivateEndpoint(aiplatform.PrivateEndpoint):\n",
    "    from typing import Optional, Dict, Any\n",
    "    _SUCCESSFUL_HTTP_RESPONSE = 300\n",
    "    _TIMEOUT =  urllib3.Timeout(10)\n",
    "    def setTimeout(self, seconds: float):\n",
    "        #Sets total timeout, you can separate connection and read\n",
    "        self._TIMEOUT = urllib3.Timeout(seconds)\n",
    "    def _http_request(\n",
    "        self,\n",
    "        method: str,\n",
    "        url: str,\n",
    "        body: Optional[Dict[Any, Any]] = None,\n",
    "        headers: Optional[Dict[str, str]] = None,\n",
    "    ) -> \"urllib3.response.HTTPResponse\":  # type: ignore # noqa: F821\n",
    "        try:\n",
    "            response = self._http_client.request(\n",
    "                method=method, url=url, body=body, headers=headers, timeout=self._TIMEOUT\n",
    "            )\n",
    "            if response.status < self._SUCCESSFUL_HTTP_RESPONSE:\n",
    "                return response\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    f\"{response.status} - Failed to make request, see response: \"\n",
    "                    + response.data.decode(\"utf-8\")\n",
    "                )\n",
    "        except urllib3.exceptions.MaxRetryError as exc:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to make a {method} request to this URI, make sure: \"\n",
    "                \" this call is being made inside the network this PrivateEndpoint is peered to \"\n",
    "                f\"({self._gca_resource.network}), calling health_check() returns True, \"\n",
    "                f\"and that {url} is a valid URL.\"\n",
    "            ) from exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6815b-031f-4e57-b5f8-e5cac7ce6515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = CustomPrivateEndpoint(\"[ENDPOINT ID]\")\n",
    "endpoint.setTimeout(1)\n",
    "response = endpoint.raw_predict(body=json.dumps(prediction_input, indent=2).encode('utf-8'), headers={'Content-Type':'application/json'}, \n",
    "                               endpoint_override=IP_ADDRESS)\n",
    "print(json.loads(response.data)['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418129bd-7910-4649-ae92-4c34fbb487f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "ENDPOINT_ID = \"[ENDPOINT ID]\"\n",
    "endpoint = aiplatform.PrivateEndpoint(ENDPOINT_ID)\n",
    "endpoint.undeploy_all()\n",
    "endpoint.delete()\n",
    "model_v1.delete()\n",
    "model_v2.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a336068-3cac-4f74-b90a-b29eb68f7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud compute forwarding-rules delete op-psc-endpoint --region={LOCATION}  --quiet\n",
    "! gcloud compute addresses delete psc-prediction --region={LOCATION} --quiet"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu118.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118:m124"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
